# ProbSwap: A Probability-Based Attack on LLM Watermarks

ProbSwap is an attack method targeting LLM watermarks by identifying and replacing low-probability tokens that are likely introduced by watermarking mechanisms.

## Core Idea

LLM watermarking typically works by modifying the sampling probability distribution during text generation to embed statistical patterns. ProbSwap attacks these watermarks by:

1. Identifying tokens that have relatively low generation probability
2. Replacing these tokens with semantically similar alternatives that have higher probability or appear more natural to another LLM

## Installation

```bash
pip install -r requirements.txt
```

## Project Structure

- `probswap/`
  - `attack.py`: Core implementation of the ProbSwap attack
  - `models.py`: Interface with different LLM models
  - `utils.py`: Utility functions
  - `evaluation.py`: Evaluation metrics and experiments

## Usage

### Basic Usage

```python
from probswap.attack import ProbSwapAttack
from probswap.models import ModelWrapper

# Initialize models
target_model = ModelWrapper("your-watermarked-model-name")
substitute_model = ModelWrapper("your-substitute-model-name")

# Initialize attack
attack = ProbSwapAttack(
    target_model=target_model.model,
    target_tokenizer=target_model.tokenizer,
    substitute_model=substitute_model.model,
    substitute_tokenizer=substitute_model.tokenizer,
    prob_threshold=0.1,  # Probability threshold for identifying suspicious tokens
    top_k_substitutes=5  # Number of replacement candidates to consider
)

# Apply attack
modified_text, modifications = attack.attack(watermarked_text)
```

### Running Examples

1. Basic attack example:
```bash
python examples/attack_example.py
```

2. Comprehensive evaluation:
```bash
python evaluation/evaluate.py
```

## Evaluation Metrics

The attack effectiveness is evaluated using several metrics:

1. Number of Modifications: Count of tokens that were replaced
2. Average Probability Increase: Average increase in token probability after replacement
3. BLEU Score: Measure of semantic similarity between original and modified text

## Integration with MarkLLM

ProbSwap can be used with watermarked text generated by [MarkLLM](https://github.com/THU-BPM/MarkLLM). Follow these steps:

1. Generate watermarked text using MarkLLM
2. Use ProbSwap to attack the watermarked text
3. Evaluate the attack effectiveness using provided metrics

## Contributing

Contributions are welcome! Please feel free to submit pull requests.

## License

MIT License
